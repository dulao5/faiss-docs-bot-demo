# build_index.py
import os
import random
from collections import defaultdict

from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI

# ------- é…ç½® -------
API_KEY = os.environ.get("OPENAI_API_KEY")
if not API_KEY:
    raise ValueError("OPENAI_API_KEY ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„")

DOCS_DIR = "docs"
FAQ_PATH = os.path.join(DOCS_DIR, "__faq_suggestions.txt")

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è¨­å®š
PER_SOURCE_MAX = 5     # ã‚½ãƒ¼ã‚¹ã”ã¨ã«æŠ½å‡ºã™ã‚‹æœ€å¤§æ•°
MAX_CHUNKS = 10        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®æœ€å¤§ãƒãƒ£ãƒ³ã‚¯æ•°
MAX_CHARS = 3000       # ç”Ÿæˆã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®æœ€å¤§æ–‡å­—æ•°
# ---------------------

print("ğŸ“‚ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹...")
loader = DirectoryLoader(
    DOCS_DIR,
    glob="**/*.md",
    loader_cls=TextLoader,
    loader_kwargs={"encoding": "utf-8"}
)
docs = loader.load()
if not docs:
    raise ValueError("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚docs ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã« .md ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")

print("âœ‚ï¸ Chunkã§åˆ†å‰²...")
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
split_docs = splitter.split_documents(docs)

print(f"ğŸ›  ãƒ™ã‚¯ã‚¿ãƒ¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆ{len(split_docs)} chunksï¼‰...")
embeddings = OpenAIEmbeddings()
db = FAISS.from_documents(split_docs, embeddings)
db.save_local("faiss_index")
print("âœ… ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ faiss_index/ ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

# ----------------- FAQ ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ -----------------
print("ğŸ¯ split_docs ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°...")
chunks_by_source = defaultdict(list)
for chunk in split_docs:
    src = chunk.metadata.get("source") or chunk.metadata.get("file_path") or "unknown"
    chunks_by_source[src].append(chunk.page_content)

sampled_parts = []
all_parts = [] # item is {"page_content": str, "source": str}

for source, parts in chunks_by_source.items():
    random.shuffle(parts)
    for part in parts:
        all_parts.append({"page_content": part, "source": source})

random.shuffle(all_parts)

# random ã®åŠ¹æœã‚’Debug
# source ã‚’å‡ºåŠ›
sourcelist = [part["source"] for part in all_parts]
sourcelist = set(sourcelist)
print(f"æŠ½å‡ºã•ã‚ŒãŸã‚½ãƒ¼ã‚¹: {sourcelist}")

# from all_parts[:MAX_CHUNKS]
sampled_parts = [part["page_content"] for part in all_parts[:MAX_CHUNKS]]

textlength = len("\n".join(sampled_parts))
combined_text = "\n".join(sampled_parts)[:MAX_CHARS]
print(f"âœ… ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Œäº†ã€‚æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®é•·ã•: {len(combined_text)} æ–‡å­— , limited by {textlength} æ–‡å­—")

# ----------------- FAQ ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ -----------------
print("ğŸ’¡ LLM ã§ ã‚µãƒ³ãƒ—ãƒ« FAQ ã‚’ç”Ÿæˆ")
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)

prompt = f"""
ã‚ãªãŸã¯FAQã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚ˆãå°‹ã­ã‚‹è³ªå•ã‚’10å€‹ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
ç”Ÿæˆã™ã‚‹è³ªå•ã¯ã€ä»¥ä¸‹ã®æ¡ä»¶ã‚’æº€ãŸã—ã¦ãã ã•ã„ï¼š
1. è³ªå•ã¯ç°¡æ½”ã§æ˜ç¢ºã«ã€‚
2. å„è³ªå•ã¯ç•°ãªã‚‹ãƒˆãƒ”ãƒƒã‚¯ã‚’ã‚«ãƒãƒ¼ã™ã‚‹ã€‚
3. è³ªå•ã¯ç®‡æ¡æ›¸ãå½¢å¼ã§ã€"- " ã¾ãŸã¯ "â€¢ " ã§å§‹ã‚ã‚‹ã€‚
4. è³ªå•ã¯æ—¥æœ¬èªã§æ›¸ãã€‚
ä»¥ä¸‹ã®å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆè³ªå•ã®ã¿ã®å‡ºåŠ›ï¼‰ï¼š
- è³ªå•1
- è³ªå•2
...
- è³ªå•10
--------------
{combined_text}
"""

resp = llm.invoke(prompt)
questions_text = resp.content.strip()

# ----------------- FAQ ã‚µãƒ³ãƒ—ãƒ«ä¿å­˜ -----------------
with open(FAQ_PATH, "w", encoding="utf-8") as f:
    for line in questions_text.splitlines():
        line = line.strip(" -â€¢\t")
        if line:
            f.write(f"- {line}\n")

print(f"âœ… FAQ ã‚µãƒ³ãƒ—ãƒ«ã‚’ {FAQ_PATH} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")