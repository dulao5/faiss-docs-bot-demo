import os
import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate

api_key = os.environ.get("OPENAI_API_KEY")
if not api_key:
    st.error("Please she the system variable OPENAI_API_KEY")
    st.stop()



@st.cache_resource
def load_qa():
    embeddings = OpenAIEmbeddings()
    db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
    retriever = db.as_retriever(search_kwargs={"k": 3})
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    return retriever, llm



retriever, llm = load_qa()

st.set_page_config(page_title="ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ãƒœãƒƒãƒˆ Demo", layout="wide")
st.title("ğŸ“š ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ãƒœãƒƒãƒˆ Demo")

faq_file = "docs/__faq_suggestions.txt"
if os.path.exists(faq_file):
    st.subheader("ğŸ’¡ FAQ ã‚µãƒ³ãƒ—ãƒ«")
    with open(faq_file, "r", encoding="utf-8") as f:
        faq_lines = [line.strip(" -â€¢\t") for line in f if line.strip()]

    col1, col2 = st.columns(2)
    for i, q in enumerate(faq_lines):
        if i % 2 == 0:
            with col1:
                if st.button(q, key=f"faq_{i}"):
                    st.session_state.query = q
        else:
            with col2:
                if st.button(q, key=f"faq_{i}"):
                    st.session_state.query = q

st.subheader("â“ è³ªå•ã‚’å…¥åŠ›")

if "query" not in st.session_state:
    st.session_state.query = ""

with st.form("query_form", clear_on_submit=False):
    query = st.text_input(
        "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
        placeholder="é¢ç™½ã„ã“ã¨ã‚’æ•™ãˆã¦ãã ã•ã„",
        key="query"
    )
    submitted = st.form_submit_button("Post")
    if submitted and query.strip():
        with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­..."):
            docs = retriever.get_relevant_documents(query)
            # æ„é€ ç›¸å…³æ€§åˆ¤æ–­ prompt
            context = "\n\n".join([doc.page_content for doc in docs])
            relevance_prompt = (
                "ä»¥ä¸‹ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã¨ã€çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æŠ½å‡ºã—ãŸ3ã¤ã®å›ç­”å€™è£œã§ã™ã€‚\n"
                "è³ªå•: {query}\n"
                "å›ç­”å€™è£œ:\n{context}\n"
                "ã“ã®ä¸­ã«è³ªå•ã¨é–¢ä¿‚æ€§ãŒã‚ã‚‹å›ç­”ãŒ1ã¤ã§ã‚‚ã‚ã‚Œã°ã€Œã¯ã„ã€ã€å…¨ããªã‘ã‚Œã°ã€Œã„ã„ãˆã€ã¨ã ã‘ç­”ãˆã¦ãã ã•ã„ã€‚"
            )
            prompt = relevance_prompt.format(query=query, context=context)
            relevance_msg = llm.invoke(prompt)
            relevance = relevance_msg.content.strip()
            if relevance == "ã„ã„ãˆ":
                st.markdown("**é–¢ä¿‚æ€§ã‚ã‚‹çµæœãŒã‚ã‚Šã¾ã›ã‚“**")
            else:
                # ç»§ç»­ç”¨ RetrievalQA ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
                answer = RetrievalQA.from_chain_type(llm, retriever=retriever).run(query)
                st.markdown(f"**å›ç­”ï¼š** {answer}")