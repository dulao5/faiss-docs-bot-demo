import os
import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import PromptTemplate
from build_index import watch_and_rebuild, build_faiss_index, build_faq_samples
import threading
import time

# Check for OpenAI API key
api_key = os.environ.get("OPENAI_API_KEY")
if not api_key:
    st.error("Please she the system variable OPENAI_API_KEY")
    st.stop()

# Initialize session state
if 'observer' not in st.session_state:
    st.session_state.observer = None

if 'last_rebuild_time' not in st.session_state:
    st.session_state.last_rebuild_time = time.time()

def rebuild_all():
    """Rebuild all resources: FAISS index, FAQ suggestions, and reload cache"""
    print("\nğŸ”„ Rebuilding all resources...")
    try:
        # 1. Build FAISS index
        db = build_faiss_index(
            docs_dir="docs",
            save_path="faiss_index"
        )
        
        # 2. Build FAQ suggestions
        build_faq_samples(
            split_docs=db.docstore._dict.values(),
            faq_path="docs/__faq_suggestions.txt"
        )
        
        # 3. Update timestamp and clear cache
        st.session_state.last_rebuild_time = time.time()
        st.cache_resource.clear()
        print("âœ… All resources rebuilt and cache cleared")
    except Exception as e:
        print(f"âŒ Error during rebuild: {e}")

# Start file watcher
if st.session_state.observer is None:
    try:
        observer = watch_and_rebuild(
            docs_dir="docs",
            faiss_index_path="faiss_index",
            cooldown=5.0
        )
        st.session_state.observer = observer
        print("ğŸ‘€ Started watching for document changes...")
    except Exception as e:
        print(f"âŒ Error starting watcher: {e}")

# Register cleanup using atexit
import atexit

def cleanup():
    if hasattr(st.session_state, 'observer') and st.session_state.observer:
        try:
            st.session_state.observer.stop()
            st.session_state.observer.join()
            print("\nğŸ‘‹ Stopped watching for changes")
        except Exception as e:
            print(f"âŒ Error during cleanup: {e}")

atexit.register(cleanup)

@st.cache_resource
def load_qa():
    # Add timestamp dependency to force reload when rebuilt
    _ = st.session_state.last_rebuild_time
    
    print("ğŸ“š Loading QA resources...")
    embeddings = OpenAIEmbeddings()
    db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
    retriever = db.as_retriever(search_kwargs={"k": 3})
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    
    # è‡ªå®šä¹‰ Prompt
    template = """
        ä»¥ä¸‹ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã¨ã€çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æŠ½å‡ºã—ãŸè¤‡æ•°ã®å‚è€ƒæƒ…å ±ã§ã™ã€‚
        ã“ã®ä¸­ã«è³ªå•ã¨é–¢ä¿‚æ€§ãŒã‚ã‚‹å›ç­”ãŒå…¨ããªã‘ã‚Œã°ã€Œã„ã„ãˆã€ã¨ã ã‘ç­”ãˆã¦ãã ã•ã„ã€‚
        é–¢ä¿‚æ€§ãŒã‚ã‚Œã°ã€è³ªå•ã«ç­”ãˆã‚‹éš›ã¯ã€å¿…ãš"ï¼ˆå‡ºå…¸:å‚è€ƒæƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«åï¼‰"ã‚’ä½¿ã£ã¦æ ¹æ‹ ã‚’æ˜ç¤ºã—ã¦ãã ã•ã„ã€‚
        è³ªå•: {query}\n"
        å‚è€ƒæƒ…å ±:\n{context}\n"
    """
    promptTempl = PromptTemplate(
        input_variables=["context", "query"],
        template=template
    )

    return retriever, llm, promptTempl

def build_context_with_sources(docs):
    """æŠŠæ–‡æ¡£å’Œæ¥æºç»„åˆæˆä¸Šä¸‹æ–‡"""
    context_parts = []
    for d in docs:
        source = d.metadata.get("source", "æœªçŸ¥")
        context_parts.append(f"{d.page_content}\n(å‡ºå…¸:{source})")
    return "\n\n".join(context_parts)


retriever, llm, prompt = load_qa()

st.set_page_config(page_title="ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ãƒœãƒƒãƒˆ Demo", layout="wide")
st.title("ğŸ“š ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ãƒœãƒƒãƒˆ Demo")
st.markdown("__è‡ªç„¶è¨€èªã§è³ªå•ã—ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰å›ç­”ã‚’å¾—ã‚‹__")

faq_file = "docs/__faq_suggestions.txt"
if os.path.exists(faq_file):
    st.subheader("ğŸ’¡ FAQ ã‚µãƒ³ãƒ—ãƒ«")
    with open(faq_file, "r", encoding="utf-8") as f:
        faq_lines = [line.strip(" -â€¢\t") for line in f if line.strip()]

    col1, col2 = st.columns(2)
    for i, q in enumerate(faq_lines):
        if i % 2 == 0:
            with col1:
                if st.button(q, key=f"faq_{i}"):
                    st.session_state.query = q
        else:
            with col2:
                if st.button(q, key=f"faq_{i}"):
                    st.session_state.query = q

st.subheader("â“ è³ªå•ã‚’å…¥åŠ›")

if "query" not in st.session_state:
    st.session_state.query = ""

with st.form("query_form", clear_on_submit=False):
    query = st.text_input(
        "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
        placeholder="é¢ç™½ã„ã“ã¨ã‚’æ•™ãˆã¦ãã ã•ã„",
        key="query"
    )
    submitted = st.form_submit_button("Post")
    if submitted and query.strip():
        with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­..."):
            docs = retriever.invoke(query)
            context = build_context_with_sources(docs)
            answer_msg = llm.invoke(prompt.format(context=context, query=query))
            answer = answer_msg.content.strip()
            debug_info = f"è³ªå•: {query}\n\nå‚è€ƒæƒ…å ±:\n{context}\n\nå›ç­”: {answer}"


            if answer == "ã„ã„ãˆ":
                st.markdown("**é–¢é€£ã™ã‚‹çµæœã¯ã‚ã‚Šã¾ã›ã‚“**")
            else:
                st.markdown(f"**å›ç­”ï¼š** {answer}")
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
            st.markdown(f"""
            <details>
                <summary>â€» Debug Info </summary>
                <code>
                {debug_info}
                </code>
            </details>
            """, unsafe_allow_html=True)


